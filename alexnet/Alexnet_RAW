{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOqVRGIZ5Blp2vjJQyl3Sap"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from PIL import Image\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Custom Dataset Class\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir)]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('L')  # Convert to grayscale\n","        label = 0 if 'COVID' in img_path else 1  # Assuming 'COVID' as class 0, 'Pneumonia' as class 1\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label, img_path  # Returning image path for visualization\n","\n","# Define Transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n","    transforms.ToTensor(),\n","])\n","\n","# Create Datasets\n","covid_train_dataset = CustomDataset(root_dir=\"/content/drive/My Drive/Colab Notebooks/Image_Processing_Project/COVID_RAW/train\", transform=transform)\n","pneumonia_train_dataset = CustomDataset(root_dir=\"/content/drive/My Drive/Colab Notebooks/Image_Processing_Project/Pneumonia_RAW/train\", transform=transform)\n","covid_test_dataset = CustomDataset(root_dir=\"/content/drive/My Drive/Colab Notebooks/Image_Processing_Project/COVID_RAW/test\", transform=transform)\n","pneumonia_test_dataset = CustomDataset(root_dir=\"/content/drive/My Drive/Colab Notebooks/Image_Processing_Project/Pneumonia_RAW/test\", transform=transform)\n","\n","# Print unique image paths in the training dataset\n","unique_train_paths = set(covid_train_dataset.image_paths + pneumonia_train_dataset.image_paths)\n","print(\"Unique training image paths:\", len(unique_train_paths))\n","\n","# Combine Datasets\n","train_dataset = torch.utils.data.ConcatDataset([covid_train_dataset, pneumonia_train_dataset])\n","test_dataset = torch.utils.data.ConcatDataset([covid_test_dataset, pneumonia_test_dataset])\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Print file paths and labels for the first few samples\n","for i in range(5):\n","    image, label, img_path = train_dataset[i]\n","    print(f\"File Path: {img_path}, Label: {label}\")\n","\n","# Check dataset lengths\n","print(\"Length of Train Dataset:\", len(train_dataset))\n","print(\"Length of Test Dataset:\", len(test_dataset))\n","\n","# Define AlexNet Model\n","alexnet = models.alexnet(pretrained=True)\n","\n","# Modify the first layer to accept a single-channel input\n","alexnet.features[0] = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n","\n","# Modify the last layer to match the number of classes (2 in this case)\n","num_ftrs = alexnet.classifier[6].in_features\n","alexnet.classifier[6] = nn.Linear(num_ftrs, 2)\n","\n","# Define Loss Function and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n","\n","# Training Loop with tqdm progress bar\n","num_epochs = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","alexnet.to(device)\n","\n","for epoch in range(num_epochs):\n","    alexnet.train()\n","    running_loss = 0.0\n","    for inputs, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = alexnet(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n","\n","# Evaluation\n","alexnet.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for inputs, labels, _ in tqdm(test_loader, desc=\"Testing\"):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        outputs = alexnet(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = correct / total\n","print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WfEj_6SxbHX","executionInfo":{"status":"ok","timestamp":1701578681386,"user_tz":-540,"elapsed":1350854,"user":{"displayName":"윤재영","userId":"15924818656922945946"}},"outputId":"09281343-d93c-4ccc-ecdb-2f6f2e1f8a9f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Unique training image paths: 2000\n","File Path: /content/drive/My Drive/Colab Notebooks/Image_Processing_Project/COVID_RAW/train/COVID_crop_result_49.png, Label: 0\n","File Path: /content/drive/My Drive/Colab Notebooks/Image_Processing_Project/COVID_RAW/train/COVID_crop_result_3.png, Label: 0\n","File Path: /content/drive/My Drive/Colab Notebooks/Image_Processing_Project/COVID_RAW/train/COVID_crop_result_68.png, Label: 0\n","File Path: /content/drive/My Drive/Colab Notebooks/Image_Processing_Project/COVID_RAW/train/COVID_crop_result_183.png, Label: 0\n","File Path: /content/drive/My Drive/Colab Notebooks/Image_Processing_Project/COVID_RAW/train/COVID_crop_result_66.png, Label: 0\n","Length of Train Dataset: 2000\n","Length of Test Dataset: 370\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n","100%|██████████| 233M/233M [00:03<00:00, 77.6MB/s]\n","Epoch 1/10: 100%|██████████| 63/63 [13:12<00:00, 12.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.5963208129008611\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10: 100%|██████████| 63/63 [00:08<00:00,  7.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, Loss: 0.2860128607541796\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10: 100%|██████████| 63/63 [00:07<00:00,  8.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, Loss: 0.1701675286841771\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10: 100%|██████████| 63/63 [00:08<00:00,  7.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10, Loss: 0.15519773138184395\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10: 100%|██████████| 63/63 [00:07<00:00,  8.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10, Loss: 0.18303879459817257\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10: 100%|██████████| 63/63 [00:08<00:00,  7.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10, Loss: 0.10190782725574478\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10: 100%|██████████| 63/63 [00:07<00:00,  8.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/10, Loss: 0.05999024318427675\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10: 100%|██████████| 63/63 [00:08<00:00,  7.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/10, Loss: 0.05841915002684035\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10: 100%|██████████| 63/63 [00:07<00:00,  8.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/10, Loss: 0.061674060595650525\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10: 100%|██████████| 63/63 [00:08<00:00,  7.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/10, Loss: 0.05376812986408671\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 12/12 [04:43<00:00, 23.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 98.65%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}